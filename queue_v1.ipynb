{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "queue_v1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO2RUqPKO1eWjhLhfAX0Lh1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/majidraeis/RL_for_Queueing/blob/master/queue_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw1TTQhCSIeZ",
        "colab_type": "text"
      },
      "source": [
        "## Queue environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFYm4nEUaS0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import random\n",
        "import tkinter as tk\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.use('TkAgg')\n",
        "from matplotlib import style\n",
        "from matplotlib.figure import Figure\n",
        "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
        "\n",
        "style.use(\"ggplot\")\n",
        "customer_color = '#191970'\n",
        "line_color = '#00BFFF'\n",
        "\n",
        "class QueueEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Define a multi-server queue.\n",
        "    The environment defines the admission control problem in a multi-server queue.\n",
        "    Action: accept(1) or reject(0), State: 1-queue length 2-#of busy servers\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_s, rho, QL_th):\n",
        "        #         self.__version__ = \"0.1.0\"\n",
        "        # General variables defining the environment\n",
        "        self.n_servers = n_s\n",
        "        self.rho = rho\n",
        "        self.QL_th = QL_th\n",
        "        self.n_jobs = 0\n",
        "        self.ql_vec = [0]\n",
        "        self.t_arr = 0  # First arrival time is 0 by default\n",
        "        self.t_vec = [0.0]\n",
        "        self.render_initiate = True\n",
        "        self.action_space = spaces.Discrete(2)\n",
        "        self.observation_space = spaces.Discrete(1000)\n",
        "        self.empty_servers = np.arange(n_s)\n",
        "        self.assigned_servers = []\n",
        "        self.t_fin = []\n",
        "        self.job_dict = {}\n",
        "        self.job_dict[0] = {'Tw': 0.0}\n",
        "        self.accepted_job_ind_vec = []\n",
        "        self.waiting_vec = []\n",
        "        self.job_index = 1\n",
        "        self.cnt = 1\n",
        "        self.MAX_STEPS = 10000\n",
        "        self.last_entered_job = 0\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        The agent takes a step in the environment.\n",
        "        Parameters\n",
        "        ----------\n",
        "        action : int\n",
        "        Returns\n",
        "        -------\n",
        "        ob, reward, episode_over, info : tuple\n",
        "            ob (object) :\n",
        "                an environment-specific object representing your observation of\n",
        "                the environment.\n",
        "            reward (float) :\n",
        "                amount of reward achieved by the previous action. The scale\n",
        "                varies between environments, but the goal is always to increase\n",
        "                your total reward.\n",
        "            episode_over (bool) :\n",
        "                whether it's time to reset the environment again. Most (but not\n",
        "                all) tasks are divided up into well-defined episodes, and done\n",
        "                being True indicates the episode has terminated. (For example,\n",
        "                perhaps the pole tipped too far, or you lost your last life.)\n",
        "            info (dict) :\n",
        "                 diagnostic information useful for debugging. It can sometimes\n",
        "                 be useful for learning (for example, it might contain the raw\n",
        "                 probabilities behind the environment's last state change).\n",
        "                 However, official evaluations of your agent are not allowed to\n",
        "                 use this for learning.\n",
        "        \"\"\"\n",
        "        self._take_action(action)\n",
        "        reward = self._get_reward(action)\n",
        "        ob = self.n_jobs\n",
        "        done = False if self.cnt < self.MAX_STEPS else True\n",
        "        self.cnt += 1\n",
        "        return ob, reward, done, {}\n",
        "\n",
        "    def _take_action(self, action):\n",
        "        # Queue length before taking the action (upon job arrival)\n",
        "        self.ql = max(self.n_jobs - self.n_servers, 0)\n",
        "        if action:\n",
        "            if self.n_jobs < self.n_servers:\n",
        "                t_ent = self.t_arr\n",
        "                self.empty_servers = [x for x in range(self.n_servers) if x not in self.assigned_servers]\n",
        "                self.assigned_servers = np.append(self.assigned_servers, random.choice(self.empty_servers))\n",
        "\n",
        "            else:\n",
        "                # finding the time that each server gets empty\n",
        "                t_available = [np.max(self.t_fin[self.assigned_servers == i]) for i in range(self.n_servers)]\n",
        "                # pick the earliest server available\n",
        "                picked_server = np.argmin(t_available)\n",
        "                t_ent = max(self.t_arr, t_available[picked_server])\n",
        "                self.assigned_servers = np.append(self.assigned_servers, picked_server)\n",
        "\n",
        "            t_s = self._service_gen()\n",
        "            self.t_fin = np.append(self.t_fin, t_ent + t_s)\n",
        "            self.n_jobs += 1\n",
        "            self.job_dict[self.job_index] = {'Ta': self.t_arr, 'Td': t_ent + t_s, 'Ts': t_s, 'Tw': t_ent-self.t_arr,\n",
        "                                             'Ba': self.ql}\n",
        "            self.last_entered_job = self.job_index\n",
        "\n",
        "        self.last_t_arr = self.t_arr\n",
        "        self.t_arr += self._inter_arr_gen()\n",
        "        served_jobs_ind = np.arange(len(self.t_fin))[np.array(self.t_fin) < self.t_arr]\n",
        "        if len(np.array(env.t_fin) < env.t_arr):\n",
        "            self.last_t_fins = self.t_fin[np.array(self.t_fin) < self.t_arr]\n",
        "        else:\n",
        "            self.last_t_fins = []\n",
        "        self.n_jobs -= np.sum(np.array(self.t_fin) < self.t_arr)\n",
        "        self.t_fin = np.delete(self.t_fin, served_jobs_ind)\n",
        "        self.assigned_servers = np.delete(self.assigned_servers, served_jobs_ind)\n",
        "        self.job_index += 1\n",
        "\n",
        "    def _inter_arr_gen(self):\n",
        "        lambda_a = self.n_servers * self.rho\n",
        "        return np.random.exponential(1 / lambda_a)\n",
        "\n",
        "    def _service_gen(self):\n",
        "        lambda_s = 1.0\n",
        "        return np.random.exponential(1 / lambda_s)\n",
        "\n",
        "    def _get_reward(self, action):\n",
        "        # Queue length after taking the action (right before the next arrival)\n",
        "        self.ql = max(self.n_jobs - self.n_servers, 0)\n",
        "        if action:\n",
        "            if self.ql > self.QL_th:\n",
        "                return -1000\n",
        "            else:\n",
        "                return 5\n",
        "        else:\n",
        "            return -5\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the state of the environment and returns an initial observation.\n",
        "        Returns\n",
        "        -------\n",
        "        observation (object): the initial observation of the space.\n",
        "        \"\"\"\n",
        "        self.ql_vec = [0]\n",
        "        self.n_jobs = 0\n",
        "        self.t_arr = 0  # First arrival time is 0 by default\n",
        "        self.t_vec = [0]\n",
        "        self.empty_servers = np.arange(self.n_servers)\n",
        "        self.assigned_servers = []\n",
        "        self.t_fin = []\n",
        "        self.job_dict = {}\n",
        "        self.job_index = 1\n",
        "        self.accepted_job_ind_vec = []\n",
        "        self.waiting_vec = []\n",
        "        self.job_dict[0] = {'Tw': 0.0}\n",
        "        self.last_entered_job = 0\n",
        "        return self.n_jobs\n",
        "\n",
        "    def _text(self):\n",
        "        text = \"QL = %d\" % self.ql\n",
        "        self.canvas.itemconfig(self.text, text=text)\n",
        "        self.canvas.update\n",
        "\n",
        "    def render(self, mode='human', close=False):  # rendering the queue after taking the action\n",
        "\n",
        "        self._Queue_plot()\n",
        "        self._QL_plot()\n",
        "        self._waiting_plot()\n",
        "        self._text()\n",
        "        return\n",
        "\n",
        "    def _Queue_plot(self):\n",
        "\n",
        "        num_busy_servers = max(self.n_jobs - self.ql, 0)\n",
        "        qHead = 300\n",
        "        qTail = 150\n",
        "        p2qDist = 2\n",
        "        c2cDist = 4\n",
        "        shift_up = 70\n",
        "        cirR_o = 25  # Server circle outer radius\n",
        "        cirR_i = 22  # Server circle inner radius\n",
        "        cirC = [335, 200]\n",
        "        linewidth = 2\n",
        "        cirC[1] = cirC[1] - ((self.n_servers - 1) * cirR_o + (self.n_servers - 1) * c2cDist / 2)\n",
        "\n",
        "        if self.render_initiate:\n",
        "            self.root = tk.Tk()\n",
        "            self.canvas = tk.Canvas(self.root, width=550, height=250)\n",
        "            self.root.title('Queueing')\n",
        "            self.canvas.pack()\n",
        "            self.inner_circle = []\n",
        "            self.canvas.create_line(qTail, 175-shift_up, qHead, 175-shift_up, width=linewidth, fill=line_color)\n",
        "            self.canvas.create_line(qTail, 225-shift_up, qHead, 225-shift_up, width=linewidth, fill=line_color)\n",
        "            self.canvas.create_line(qHead, 175-shift_up, qHead, 225-shift_up, width=linewidth, fill=line_color)\n",
        "            for c in range(self.n_servers):\n",
        "                self.canvas.create_oval(cirC[0] - cirR_o, cirC[1] - cirR_o-shift_up, cirC[0] + cirR_o,\n",
        "                                        cirC[1] + cirR_o-shift_up, outline=line_color, width=linewidth)\n",
        "                self.inner_circle.append(self.canvas.create_oval(cirC[0] - cirR_i, cirC[1] - cirR_i-shift_up, cirC[0]\n",
        "                                         + cirR_i, cirC[1] + cirR_i-shift_up, outline='white',\n",
        "                                         fill='white', width=linewidth))\n",
        "                cirC[1] = cirC[1] + c2cDist + 2 * cirR_o\n",
        "            self.queue_len = self.canvas.create_rectangle(qHead - p2qDist-p2qDist, 180-shift_up, qHead - p2qDist-p2qDist\n",
        "                                                          , 220-shift_up, fill=customer_color)\n",
        "\n",
        "            self.text = self.canvas.create_text((qTail+qHead)/2, 166-shift_up, fill=\"black\", text=\"Queue length = 0\")\n",
        "        else:\n",
        "\n",
        "            self.canvas.coords(self.queue_len, qHead - p2qDist-p2qDist - self.ql * 5, 180-shift_up,\n",
        "                               qHead - p2qDist-p2qDist, 220-shift_up)\n",
        "\n",
        "            for c in range(self.n_servers):\n",
        "                if c+1 <= num_busy_servers:\n",
        "                    self.canvas.itemconfig(self.inner_circle[c], fill=customer_color)\n",
        "                    self.canvas.update()\n",
        "\n",
        "                else:\n",
        "                    self.canvas.itemconfig(self.inner_circle[c], fill='white', width=linewidth)\n",
        "                    self.canvas.update()\n",
        "\n",
        "                cirC[1] = cirC[1] + c2cDist + 2 * cirR_o\n",
        "\n",
        "    def _QL_plot(self):\n",
        "        # **************After taking the action*************\n",
        "        # ***************** Queue Length *******************\n",
        "        n_job_after_action = self.n_jobs + len(self.last_t_fins)\n",
        "        ql_after_action = max(n_job_after_action - self.n_servers, 0)\n",
        "        self.ql_vec = np.append(self.ql_vec, ql_after_action)\n",
        "        self.t_vec = np.append(self.t_vec, self.last_t_arr)\n",
        "        # ******** Departures until the next arrival **********\n",
        "        ql_deps = n_job_after_action - np.arange(1, len(self.last_t_fins)+1)\n",
        "        ql_deps = (ql_deps - self.n_servers) * (ql_deps - self.n_servers > 0)\n",
        "        self.ql_vec = np.append(self.ql_vec, ql_deps)\n",
        "        self.t_vec = np.append(self.t_vec, np.sort(self.last_t_fins))\n",
        "\n",
        "        if self.render_initiate:\n",
        "\n",
        "            self.figure1 = Figure(figsize=(5, 4))\n",
        "            self.subplot1 = self.figure1.add_subplot(111)\n",
        "            # Queue length after taking the action\n",
        "            self.subplot1.step(self.t_vec, self.ql_vec, where='post', color='lightsteelblue')\n",
        "            self.plot1 = FigureCanvasTkAgg(self.figure1, self.root)\n",
        "            self.plot1.get_tk_widget().pack(side=tk.LEFT, fill=tk.BOTH, expand=1)\n",
        "            self.subplot1.set_xlabel('Time')\n",
        "            self.subplot1.set_ylabel('Queue length')\n",
        "        else:\n",
        "\n",
        "            self.subplot1.clear()\n",
        "            self.subplot1.step(self.t_vec, self.ql_vec, where='post', color='tab:blue')\n",
        "            self.figure1.canvas.draw_idle()\n",
        "            self.subplot1.set_xlabel('Time')\n",
        "            self.subplot1.set_ylabel('Queue length')\n",
        "\n",
        "    def _waiting_plot(self):\n",
        "        # ***************** Waiting time ********************\n",
        "        if self.render_initiate:\n",
        "\n",
        "            self.accepted_job_ind_vec.append(self.last_entered_job)\n",
        "            self.waiting_vec.append(self.job_dict[self.last_entered_job]['Tw'])\n",
        "            self.figure2 = Figure(figsize=(5, 4))\n",
        "            self.subplot2 = self.figure2.add_subplot(111)\n",
        "            self.subplot2.plot(self.accepted_job_ind_vec, self.waiting_vec, color='tab:red')\n",
        "            self.subplot2.set_xlabel('Job')\n",
        "            self.subplot2.set_ylabel('Waiting time')\n",
        "            self.plot2 = FigureCanvasTkAgg(self.figure2, env.root)\n",
        "            self.plot2.get_tk_widget().pack(side=tk.LEFT, fill=tk.BOTH, expand=1)\n",
        "            self.render_initiate = False  # This must be run in the last plot function\n",
        "        else:\n",
        "            self.accepted_job_ind_vec.append(self.last_entered_job)\n",
        "            self.waiting_vec.append(self.job_dict[self.last_entered_job]['Tw'])\n",
        "            self.subplot2.clear()\n",
        "            self.subplot2.plot(self.accepted_job_ind_vec, self.waiting_vec, color='tab:red')\n",
        "            self.subplot2.set_xlabel('Job')\n",
        "            self.subplot2.set_ylabel('Waiting time')\n",
        "            self.figure2.canvas.draw_idle()\n",
        "\n",
        "\n",
        "class Agent():\n",
        "    def __init__(self, env):\n",
        "        self.isCont = \\\n",
        "            type(env.action_space) == gym.spaces.box.Box\n",
        "        if self.isCont:\n",
        "            self.action_low = env.action_space.low\n",
        "            self.action_high = env.action_space.high\n",
        "            self.action_shape = env.action_space.shape\n",
        "\n",
        "        else:\n",
        "            self.action_size = env.action_space.n\n",
        "            print(\"Action size:\", self.action_size)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        if self.isCont:\n",
        "            action = np.random.uniform(self.action_low, self.action_high, self.action_shape)\n",
        "        else:\n",
        "            action = random.choice(range(env.action_space.n))\n",
        "        return action\n",
        "\n",
        "class QAgent(Agent):\n",
        "    def __init__(self, env, discount_rate=0.97, learning_rate=0.01):\n",
        "        super().__init__(env)\n",
        "        self.state_size = env.observation_space.n\n",
        "\n",
        "        self.discount_rate = discount_rate\n",
        "        self.learning_rate = learning_rate\n",
        "        self.eps = 0.98\n",
        "        self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        self.q_table = 1e-4 * np.random.random((self.state_size, self.action_size))\n",
        "\n",
        "    def get_action(self, state):\n",
        "        q_state = self.q_table[state]\n",
        "        action_greedy = np.argmax(q_state)\n",
        "        action_random = super().get_action(state)\n",
        "        return action_random if random.random() < self.eps else action_greedy\n",
        "\n",
        "    def train(self, experience):\n",
        "        state, action, next_state, reward, done = experience\n",
        "\n",
        "        q_next = self.q_table[next_state]\n",
        "        q_next = np.zeros([self.action_size]) if done else q_next\n",
        "        q_target = reward + self.discount_rate * np.max(q_next)\n",
        "\n",
        "        q_update = q_target - self.q_table[state, action]\n",
        "        self.q_table[state, action] += self.learning_rate * q_update\n",
        "\n",
        "        if done:\n",
        "            self.eps *= 0.99\n",
        "\n",
        "\n",
        "#******************  Demo  ********************\n",
        "#**********************************************\n",
        "\n",
        "render_flag = True\n",
        "env = QueueEnv(3, 3, 10)\n",
        "agent = QAgent(env)\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    action = agent.get_action(state)\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    agent.train((state, action, next_state, reward, done))\n",
        "    state = next_state\n",
        "    print(\"s:\", state, \"a:\", action)\n",
        "    if render_flag:\n",
        "        env.render()\n",
        "\n",
        "if render_flag:\n",
        "    env.canvas.mainloop()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}